# otus-qa-course
This repository is for learning QA course by OTUS

# dz20 Paramiko/SSH
Результат работы в модуле ssh_paramiko/test_server_access.py

# dz19 Database using
Результат работы в папке tests/opencartadmin.
В тестовом модуле test_admin_products_page.py создана новая функция add_new_product_through_db().
Используется для добавления продукта через базу для теста редактирования продукта.
В тест удаления продукта добавлена проверка, что удаленного продукта действительно нет в бд.

# dz18 Log Analysis
Лог access.log и скрипт его анализа находятся в папке log_analysis.

Чтобы запустить скрипт анализа логов, необходимо выполнить python log_analysis/log_analysis.py -f=путь/до/файла/с/логами
или -f=путь/до/директории/с/файлами/логов.
Примеры:
- python log_analysis/log_analysis.py -f=log_analysis/
или
- python log_analysis/log_analysis.py -f=log_analysis/access.log

Скрипт создаст json-файл log_analysis_results.json в директории log_analysis/
с результатами анализа по каждому файлу, если их несколько.

# dz17 Allure
Файлы отчета Allure находятся в папке allure_report.
Скриншоты отчета находятся в папке allure_screenshots.

# dz16 Selenoid
Тесты для запуска в Selenoid находятся в модуле selenoid/test_for_selenoid.py
Скриншоты результатов тестов в папке selenoid.
Был добавлен один новый браузер Chrome и запущены тесты в нем.

# dz15 Selenium Grid
Тесты для запуска в Selenium Grid'е находятся в модуле seleniumserver/test_opencart.py
Скриншоты результатов запусков можно найти в папке seleniumserver/screenshots

Тест для прогона на browserstack.com находится в модуле seleniumserver/test_using_browserstack
Скриншот можно найти в папке seleniumserver/screenshots/browserstack.png 

# dz14 logging
Для того, чтобы логи сохранились в файле, указать параметр --logfile="<имя файла>"
Чтобы логи отобразились в консоли, указать параметр -s

# dz11
Запустить тесты создания, редактирования, удаления продукта в админке: $pytest -v -m=dz11
Сами тесты лежат в tests/opencartadmin/test_products_page.py

# dz9
Запустить все тесты: $pytest -v tests/*

# dz8
Запустить тест: $pytest tests/test_opencart.py [--browser] [--base_url]

По-умолчанию --browser=chrome, --base_url="http://192.168.56.101/"

Опция --browser принимает одно из трех значений: chrome, firefox, safari

----------------------------

# Моя пирамида тестирования:
О проекте: проект написан на Django, REST API, количество пользователей - около 100, разработчики - 2 бэк, 1 фронт;
т.к. система является госзаказом, разработка новых фич начинается после заключения контракта,
любые признаки scrum отсутствуют, релизы как правило объемные и редкие, но с 2020 года решили выкатывать обновления
более мелкими релизами и чаще. Также с нового года планируется разработка с использованием
фича-бранчей (до этого все разработчики работали с одной веткой).

1. UI-тесты
    - ручные: ~500 тест-кейсов, регресс идет 2 дня в 2 человека
    - автоматизированные (на тестируемом проекте): месяца два назад начали создавать тестовый фреймворк
    (используем Django + pytest + selenium + allure), пока написано только 6 автотестов и найден 1 баг с помощью них :)
2. Интеграционные тесты
    - можно сказать, что отсутствуют (есть коллекция в Postman для тестирования API, но там только по одному позитивному
     тесту на каждый endpoint)
    - во время регресса отдельно API не тестируется
    - нагрузочное тестирование не проводится
3. Unit-тесты
    - пишут как бэк-разработчики (много), так и фронт (мало, только критичные моменты).
    - измеряли покрытие python-кода в проекте средствами pycharm-а,
    показало в разных модулях 90-98% покрытия
    
Преимущества: преимуществом считаю добросовестный подход разработчиков к написанию unit-тестов

Недостатки: недостатком считаю полное отсутствие тестирования API, документация к API появилась только месяц назад,
хотя проекту уже больше года (разработать документацию было моей идеей). Когда подняла вопрос о необходимости 
тестирования API, например, в Postman, аналитики сказали, что если есть баги, которые пользователи не смогут повторить 
через UI, то они нас не интересуют, предложили заниматься тестированием API в фоновом режиме, когда нет других задач :)

Сама не знаю, как правильно поступить в такой ситуации, но считаю, что в случае изменения поведения на фронте 
(что случается не редко) могут всплывать баги бэка, которые мы могли исправить ранее, если бы тестировали API.